# -*- coding: utf-8 -*-
"""BENNETTA_AVAGA_SportsPrediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17ITmpeYTaGFijf7HPJPK6RrI0Tj89upd

1. Demonstrate the data preparation & feature extraction proces
"""

import sklearn
print(sklearn.__version__)

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
import numpy as np
import joblib # for saving models

male_players_legacy = pd.read_csv("C:\\Users\\user\\Downloads\\Sports Prediction app using ML model\\male_players (legacy).csv", low_memory = False)
players_22 = pd.read_csv("C:\\Users\\user\\Downloads\\Sports Prediction app using ML model\\players_22.csv", low_memory = False)

male_players_legacy.head()

# Cleaning up the independent dataset

# Remove duplicates (if any)
male_players_legacy = male_players_legacy.drop_duplicates()

# Dropping columns with missing values
male_players_legacy_cleaned = male_players_legacy.dropna(axis=1, how='any')

# Checking the cleaned DataFrame for confirmation
cleaned_missing_values = male_players_legacy_cleaned.isnull().sum()
cleaned_missing_values.info

male_players_legacy_cleaned.columns

"""2. Create feature subsets that show maximum correlation with the dependent variable."""

# Feature Extraction by creating a new age_group column
male_players_legacy_cleaned['age_group'] = pd.cut(
    male_players_legacy_cleaned['age'],
    bins=[0, 20, 30, 40, 100],
    labels=['<20', '20-30', '30-40', '40+']
)

# Convert age_group into numeric form for correlation
male_players_legacy_cleaned['age_group_encoded'] = male_players_legacy_cleaned['age_group'].cat.codes

numeric_df = male_players_legacy_cleaned.select_dtypes(include='number')
corr_matrix = numeric_df.corr()

# Get correlations with the target variable
target = 'overall'
correlations = corr_matrix[target].sort_values(ascending=False)

# Display top correlated features
print("Top 15 features correlated with player overall rating:\n")
print(correlations.head(15))


# Visualize top correlations (excluding the target itself)
top_features = correlations.drop(target).head(15)

plt.figure(figsize=(10,6))
sns.barplot(x=top_features.values, y=top_features.index)
plt.title(f"Top 15 Features Correlated with '{target}'")
plt.xlabel("Correlation Coefficient")
plt.ylabel("Feature")
plt.show()

# Calculate correlation matrix
# Include 'overall' + your top correlated features
corr_features = [
    'overall', 'movement_reactions', 'potential', 'attacking_short_passing',
    'mentality_vision', 'international_reputation', 'skill_long_passing',
    'power_shot_power', 'age', 'skill_ball_control', 'age_group_encoded',
    'skill_curve', 'power_long_shots', 'mentality_aggression', 'attacking_crossing'
]

corr_matrix = male_players_legacy_cleaned[corr_features].corr()

plt.figure(figsize=(12,8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title("Correlation Matrix of Top Features with Player Overall Rating", fontsize=14)
plt.show()

features = [
    'movement_reactions', 'potential', 'attacking_short_passing',
    'mentality_vision', 'international_reputation', 'skill_long_passing',
    'power_shot_power', 'age', 'skill_ball_control', 'age_group_encoded',
    'skill_curve', 'power_long_shots', 'mentality_aggression', 'attacking_crossing'
]

X = male_players_legacy_cleaned[features]
y = male_players_legacy_cleaned['overall']

"""3. Create and train a suitable machine learning model with cross-validation that can predict a player's rating."""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

models = {
    "Linear Regression": LinearRegression(),
    "Decision Tree": DecisionTreeRegressor(random_state=42),
    "Random Forest": RandomForestRegressor(random_state=42)
}

results = []
best_model = None
best_score = -np.inf  # weâ€™ll maximize RÂ²

for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    r2 = r2_score(y_test, y_pred)
    mae = mean_absolute_error(y_test, y_pred)
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)

    results.append({
        "Model": name,
        "RÂ²": r2,
        "MAE": mae,
        "MSE": mse,
        "RMSE": rmse
    })

    # Track the best model by RÂ²
    if r2 > best_score:
        best_score = r2
        best_model = model
        best_name = name

# Show results
results_df = pd.DataFrame(results)
print(results_df)

# Save the best model
joblib.dump(best_model, f"{best_name.replace(' ', '_').lower()}_model.pkl")
print(f"\nBest model '{best_name}' saved as '{best_name.replace(' ', '_').lower()}_model.pkl'")

"""Interpretation of results
Models and meaning
Random Forest ðŸŒŸ	Explains 97% of the variance (RÂ² = 0.9677). On average, itâ€™s off by about 0.83 rating points â€” very accurate. This is your best model.
Decision Tree	Explains 93% of variance. Still decent, but a bit more error (MAE â‰ˆ 1.15). Less stable â€” may overfit.
Linear Regression	Explains 90% of variance. Performs worse, meaning itâ€™s not capturing non-linear relationships as well.

Conclusion
Random Forest performs the best overall â€” it captures complex patterns in the data.

Decision Tree does well but is more likely to overfit or underfit depending on your data.

Linear Regression is the simplest and least accurate here.

4. Use the data from another season(players_22) which was not used during the training to test how good is the model.
"""

players_22.head()

# Recreate age group and encode
players_22['age_group'] = pd.cut(players_22['age'], bins=[0, 20, 30, 40, 100], labels=['<20', '20-30', '30-40', '40+'])
players_22['age_group_encoded'] = players_22['age_group'].cat.codes

# Use the same features as during training
relevant_features = [
    'movement_reactions', 'potential', 'attacking_short_passing',
    'mentality_vision', 'international_reputation', 'skill_long_passing',
    'power_shot_power', 'age', 'skill_ball_control', 'age_group_encoded',
    'skill_curve', 'power_long_shots', 'mentality_aggression',
    'attacking_crossing'
]

X_new = players_22[relevant_features]
y_new = players_22['overall']

y_pred_new = best_model.predict(X_new)

r2 = r2_score(y_new, y_pred_new)
mae = mean_absolute_error(y_new, y_pred_new)
mse = mean_squared_error(y_new, y_pred_new)
rmse = np.sqrt(mse)

print("Performance on players_22 dataset:")
print(f"RÂ²: {r2:.3f}")
print(f"MAE: {mae:.3f}")
print(f"MSE: {mse:.3f}")
print(f"RMSE: {rmse:.3f}")

"""Interpretation:

RÂ² = 0.99: Outstanding, the model generalizes extremely well to unseen data (players from a new season).
MAE = 0.397: On average, predictions are off by less than half a point in player rating.
RMSE = 0.687: Confirms that even the larger errors are small, the model is very consistent.

No overfitting warning: Since performance on the new dataset is slightly better, it likely means the new data is more predictable (similar structure, not overly different).
"""

# Train using top 5 features
import pickle

top_5_features = ['movement_reactions', 'potential', 'attacking_short_passing',
    'mentality_vision', 'international_reputation']
X_top5 =  male_players_legacy_cleaned[top_5_features]
y =  male_players_legacy_cleaned['overall']

X_train, X_test, y_train, y_test = train_test_split(X_top5, y, test_size=0.2, random_state=42)
model_top5 = RandomForestRegressor(random_state=42)
model_top5.fit(X_train, y_train)
y_pred = model_top5.predict(X_test)

print("RÂ²:", r2_score(y_test, y_pred))
print("MAE:", mean_absolute_error(y_test, y_pred))
print("RMSE:", np.sqrt(mean_squared_error(y_test, y_pred)))

with open("C:\\Users\\user\\Downloads\\Sports Prediction app using ML model\\top5_model.pkl", "wb") as f:
    pickle.dump(model_top5, f)

"""Even though I've lost some precision but gained simplicity (fewer inputs â†’ easier deployment). I'll keep the 5-feature model for a lightweight app where users input only a few values (5)
The initial model with 25 features and a higher accuracy is still in the folder
"""

